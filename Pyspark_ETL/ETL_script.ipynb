{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого задания данные выгружены из 2х различных систем, хранящих пользовательские логи. \\\n",
    "Для каждой из систем данные разбиты по дню выгрузки. \\\n",
    "В данных за разные дни нет пересечений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате ожидается, что можно будет взять сырые логи для разных аналитических систем, разбитые по дням, и привести их к общему виду с помощью первой функции. \\\n",
    "А после этого взять обработанные логи и последовательно день за днем с помощью второй функции получить итеративно увеличивающиеся день ото дня версии таблиц с девайсами. \\\n",
    "**Оба задания нужно сделать с использованием pyspark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Унификация данных из внешних аналитических систем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно написать функцию на python, которая будет принимать на вход путь к директории с логами и дату, читать логи за эту дату и унифицировать их, приводя к общей для 2х систем схеме препарированных логов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Препарированные логи должны содержать колонки:\n",
    "\n",
    "* `external_did` – идентификатор устройства из внешней системы\n",
    "* `event_name` – название эвента\n",
    "* `event_datetime`\n",
    "* `event_json` – json-строка с параметрами эвента. \\\n",
    "    Для одной из систем колонка заполнена для некоторых записей в таблице и нужно брать её из сырых логов без изменений \\\n",
    "    Для другой это не так и сюда нужно складывать user_properties + event_properties\n",
    "* `date`\n",
    "* `push_token` \\\n",
    "    Для одной из систем сюда нужно записать 'registration_id' из 'user_property' \\\n",
    "    Для другой (где этого property нет) сюда нужно записать то же значение, что в external_did\n",
    "* `ios_ifa` – идентификатор, проставляемый для ios устройства (IDFA)\n",
    "* `external_profile_id` – идентификатор юзера / профиля из внешней системы\n",
    "* `external_app_id` – application_id из сырых логов\n",
    "* `external_system`\n",
    "* `internal_app_id` – какой-нибудь вымышленный generic идентификатор приложения, чтобы отличать препарированные логи одного  приложения от другого (потребуется для следующего задания)\n",
    "* `country_iso_code` – ISO 3166-1 Alpha-2 code\n",
    "* `device_locale` – язык пользователя\n",
    "\n",
    "Можно обратить внимание, например, на https://www.geonames.org/ и доджойнить данные оттуда по Locale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт библиотек\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import os\n",
    "import sys\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path, PurePath\n",
    "from typing import Union\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Необходимо для корректной работы pyspark на windows\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание спарк сессии на локальной машине\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"Python Spark example\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_tar_gz(\n",
    "        archive_path: str, \n",
    "        destination_path: str\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Функция распаковки архивов tar.\n",
    "\n",
    "    Функция принимает на вход путь к архиву и\n",
    "    путь для извлечения файлов\n",
    "    \n",
    "    Аргументы:\n",
    "        archive_path: путь к архиву\n",
    "        destination_path: путь для извлечения файлов\n",
    "    \"\"\"\n",
    "    with tarfile.open(archive_path, 'r:gz') as tar:\n",
    "        tar.extractall(destination_path)\n",
    "\n",
    "# Распаковка архивов с данными\n",
    "destination_path = 'Q:\\Python\\data_engineer_task' # Необходимо указать свой путь\n",
    "unpack_tar_gz('raw_data_1.tar.gz', destination_path)\n",
    "unpack_tar_gz('raw_data_2.tar.gz', destination_path)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_logs(\n",
    "    external_system_name: str,\n",
    "    date: Union[str, date],\n",
    "    raw_logs_path: Union[str, Path],\n",
    "    prepared_logs_path: Union[str, Path]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    external_system_name: python-строка, название внешней системы\n",
    "    date: дата выгрузки логов. Внутри директорий с сырыми логами данные сгрупированны по дате\n",
    "    raw_logs_path: путь к директории с сырыми логами\n",
    "    prepared_logs_path: путь к директории с препарированными логами\n",
    "    \n",
    "    Внутри функции должно происходить чтение логов с помощью pyspark,\n",
    "    приведение их к общей схеме и запись в директорию с препарированными логами.\n",
    "    Препарированные логи должны быть сгрупированны по internal_app_id и date\n",
    "    \"\"\"\n",
    "    # Чтение сырых логов \n",
    "    raw_logs = spark.read.parquet(raw_logs_path, header=True)\n",
    "\n",
    "    if external_system_name == \"external_system_1\":\n",
    "       \n",
    "        # Получаем данные о странах и кодах с сайта\n",
    "        data = pd.read_html(\"http://www.geonames.org/statistics/\") \n",
    "        data = pd.DataFrame(data[1])\n",
    "        data = data[[\"Country\", \"CountryCode\"]] # Выбираем из данных необходимые столбцы\n",
    "        data.columns = [\"device_locale\", \"country_iso_code\"] # Переименуем столбцы\n",
    "        geo_table = spark.createDataFrame(data) # Преобразуем Pandas датафрейм в Pyspark\n",
    "\n",
    "        # Получаем значение application_id из пути сырых логов\n",
    "        path = PurePath(raw_logs.inputFiles()[0])\n",
    "        app_id = path.parts[-3]\n",
    "        app_id = app_id.split(\"=\")[1]\n",
    "       \n",
    "       # Преобразуем данные из первой системы\n",
    "        raw_logs = raw_logs.withColumnRenamed(\"device_id\", \"external_did\")\n",
    "        raw_logs = raw_logs.withColumnRenamed(\"event_type\", \"event_name\")\n",
    "        raw_logs = raw_logs.withColumnRenamed(\"event_time\", \"event_datetime\")\n",
    "        raw_logs = raw_logs.withColumn(\"event_json\", f.concat(\"user_properties\", \"event_properties\"))\n",
    "        raw_logs = raw_logs.withColumn(\"push_token\", f.get_json_object(raw_logs.user_properties, \"$.registration_id\"))\n",
    "        raw_logs = raw_logs.withColumnRenamed(\"idfa\", \"ios_ifa\")\n",
    "        raw_logs = raw_logs.withColumnRenamed(\"user_id\", \"external_profile_id\")\n",
    "        raw_logs = raw_logs.withColumn(\"external_app_id\", f.lit(app_id))\n",
    "        raw_logs = raw_logs.withColumn(\"internal_app_id\", f.substring(\"external_system\", -1, 1))\n",
    "        raw_logs = raw_logs.withColumn(\"device_locale\", f.substring(\"Language\", 1, 6)) \n",
    "\n",
    "        # Выбор необходимых столбцов\n",
    "        prepare_logs = raw_logs.join(geo_table, \"device_locale\").select(raw_logs.external_did,\n",
    "                                                                        raw_logs.event_name,\n",
    "                                                                        raw_logs.event_datetime,\n",
    "                                                                        raw_logs.event_json,\n",
    "                                                                        raw_logs.date,\n",
    "                                                                        raw_logs.push_token,\n",
    "                                                                        raw_logs.ios_ifa,\n",
    "                                                                        raw_logs.external_profile_id,\n",
    "                                                                        raw_logs.external_app_id,\n",
    "                                                                        raw_logs.external_system,\n",
    "                                                                        raw_logs.internal_app_id,\n",
    "                                                                        geo_table.country_iso_code,\n",
    "                                                                        raw_logs.device_locale\n",
    "                                                                        )\n",
    "        \n",
    "    if external_system_name == \"external_system_2\":\n",
    "        \n",
    "        # Получаем значение application_id из пути сырых логов\n",
    "        path = PurePath(raw_logs.inputFiles()[0])\n",
    "        app_id = path.parts[-3]\n",
    "        app_id = app_id.split(\"=\")[1]\n",
    "\n",
    "        # Преобразуем данные из второй системы\n",
    "        raw_logs = raw_logs.withColumnRenamed(\"uniq_device_id\", \"external_did\") \n",
    "        raw_logs = raw_logs.withColumn(\"push_token\", raw_logs.external_did)\n",
    "        raw_logs = raw_logs.withColumnRenamed(\"profile_id\", \"external_profile_id\")\n",
    "        raw_logs = raw_logs.withColumnRenamed(\"application_id\", \"external_app_id\")\n",
    "        raw_logs = raw_logs.withColumn(\"internal_app_id\", f.substring(\"external_system\", -1, 1))\n",
    "\n",
    "        # Выбор необходимых столбцов\n",
    "        prepare_logs = raw_logs.select(raw_logs.external_did,\n",
    "                                       raw_logs.event_name,\n",
    "                                       raw_logs.event_datetime,\n",
    "                                       raw_logs.event_json,\n",
    "                                       raw_logs.date,\n",
    "                                       raw_logs.push_token,\n",
    "                                       raw_logs.ios_ifa,\n",
    "                                       raw_logs.external_profile_id,\n",
    "                                       raw_logs.external_app_id,\n",
    "                                       raw_logs.external_system,\n",
    "                                       raw_logs.internal_app_id,\n",
    "                                       raw_logs.country_iso_code,\n",
    "                                       raw_logs.device_locale\n",
    "                                       )\n",
    "\n",
    "    prepare_logs = prepare_logs.filter(prepare_logs.date == date) # Отбираем логи по дате\n",
    "\n",
    "    # Сохранение препарированных логов в файл \n",
    "    internal_app_id = external_system_name[-1]          \n",
    "    (\n",
    "    prepare_logs.write\n",
    "                .mode(\"overwrite\")\n",
    "                .parquet(f\"{prepared_logs_path}/internal_app_id={internal_app_id}/{str(date)}\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " external_did        | 2bb2b7629ff9977c8... \n",
      " event_name          | AppRolledUp          \n",
      " event_datetime      | 2023-01-05 03:04:... \n",
      " event_json          | {\"user_property_1... \n",
      " date                | 2023-01-05           \n",
      " push_token          | ehuQJKJXpkcCt_NQt... \n",
      " ios_ifa             | null                 \n",
      " external_profile_id | 45d3abd2-6756-4eb... \n",
      " external_app_id     | application_1        \n",
      " external_system     | external_system_1    \n",
      " internal_app_id     | 1                    \n",
      " country_iso_code    | RU                   \n",
      " device_locale       | Russia               \n",
      "-RECORD 1-----------------------------------\n",
      " external_did        | 2bb2b7629ff9977c8... \n",
      " event_name          | PlaceScreen_Loaded   \n",
      " event_datetime      | 2023-01-05 03:04:... \n",
      " event_json          | {\"user_property_1... \n",
      " date                | 2023-01-05           \n",
      " push_token          | ehuQJKJXpkcCt_NQt... \n",
      " ios_ifa             | null                 \n",
      " external_profile_id | 45d3abd2-6756-4eb... \n",
      " external_app_id     | application_1        \n",
      " external_system     | external_system_1    \n",
      " internal_app_id     | 1                    \n",
      " country_iso_code    | RU                   \n",
      " device_locale       | Russia               \n",
      "-RECORD 2-----------------------------------\n",
      " external_did        | 2bb2b7629ff9977c8... \n",
      " event_name          | PlaceScreen_Appear   \n",
      " event_datetime      | 2023-01-05 03:04:... \n",
      " event_json          | {\"user_property_1... \n",
      " date                | 2023-01-05           \n",
      " push_token          | ehuQJKJXpkcCt_NQt... \n",
      " ios_ifa             | null                 \n",
      " external_profile_id | 45d3abd2-6756-4eb... \n",
      " external_app_id     | application_1        \n",
      " external_system     | external_system_1    \n",
      " internal_app_id     | 1                    \n",
      " country_iso_code    | RU                   \n",
      " device_locale       | Russia               \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepare_raw_logs(\"external_system_1\", date(year=2023, month=1, day=5), \n",
    "                 \"Q:\\Python\\data_engineer_task\\external_system_1\", \"Q:\\Python\\date\")\n",
    "\n",
    "df1 = spark.read.parquet(f\"Q:\\Python\\date\\internal_app_id=1\\{date(year=2023, month=1, day=5)}\")\n",
    "df1.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taks 2: создание таблицы девайсов с актуальным состоянием колонок из препарированных логов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation_table(\n",
    "        dataframe: object\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    Функция агрегации датафреймов.\n",
    "\n",
    "    Функция принимает на вход pyspark.DataFrame,\n",
    "    и возвращает агрегированный pyspark.DataFrame.\n",
    "\n",
    "    Аргументы:\n",
    "        dataframe: датафрейм pyspark\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = dataframe.groupBy(f.col(\"external_did\"), f.col(\"date\")).agg(\n",
    "            f.min(\"event_datetime\").alias(\"first_event_datetime\"),\n",
    "            f.max(\"event_datetime\").alias(\"last_event_datetime\"),\n",
    "            f.max(f.when(f.col(\"push_token\").isNotNull(), f.col(\"push_token\"))).alias(\"push_token\"),\n",
    "            f.max(f.when(f.col(\"ios_ifa\").isNotNull(), f.col(\"ios_ifa\"))).alias(\"ios_ifa\"),\n",
    "            f.max(f.when(f.col(\"external_profile_id\").isNotNull(), \n",
    "                        f.col(\"external_profile_id\"))).alias(\"external_profile_id\"),\n",
    "            f.max(f.when(f.col(\"external_app_id\").isNotNull(), \n",
    "                        f.col(\"external_app_id\"))).alias(\"external_app_id\")\n",
    "        )\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_devices_table(\n",
    "    internal_app_id: Union[int, str],\n",
    "    date: Union[str, date],\n",
    "    prepared_logs_path: Union[str, Path],\n",
    "    devices_path: Union[str, Path]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    date: дата препарированных логов\n",
    "    internal_app_id: идентификатор приложения\n",
    "    prepared_logs_path: путь к директории с препарированными логами\n",
    "    devices_path: путь к директории с таблицами devices\n",
    "    \n",
    "    Внутри функции должно происходить чтение препарированных логов с помощью pyspark\n",
    "    А также чтение таблицы за предыдущий день (если она есть)\n",
    "\n",
    "    В результате должна получиться таблица, содержащая обновленные данные юзеров, которые совершали эвенты за день `date`\n",
    "    Для тех юзеров, которые не совершали эвенты за предыдущий день, данные должны браться из таблицы за предыдущий день.\n",
    "    \n",
    "    Для каждого external_did нужно хранить datetime первого и последнего эвента из препарированных логов,\n",
    "    а также актуальное значение идентификаторов и токенов (последнее ненулевое), если они встречались в логах\n",
    "        \n",
    "    Внутри директории devices таблицы должны быть сгруппированы по internal_app_id и date\n",
    "    \"\"\"\n",
    "    \n",
    "    # Чтение препарированных логов\n",
    "    prep_logs = spark.read.parquet(f\"{prepared_logs_path}/internal_app_id={internal_app_id}/{str(date)}\")\n",
    "\n",
    "    # Чтение таблицы за предыдущий день (если она есть)   \n",
    "    start_day = date\n",
    "    yesterday = start_day - timedelta(days=1)\n",
    "    try:    \n",
    "        yesterday_logs = spark.read.parquet(f\"{devices_path}/internal_app_id={internal_app_id}/{str(yesterday)}\")\n",
    "    except Exception:\n",
    "        yesterday_logs = None\n",
    "        print(\"Таблица за предыдущий день не найдена\")\n",
    "\n",
    "    # Преобразование данных\n",
    "    if (prep_logs.filter(f.col(\"event_name\").isNull())).count() == 0:\n",
    "        devices_logs = prep_logs\n",
    "\n",
    "        # Агрегация таблицы    \n",
    "        devices_logs = aggregation_table(devices_logs)\n",
    "        \n",
    "    elif (prep_logs.filter(f.col(\"event_name\").isNull())).count() != 0 and yesterday_logs != None:     \n",
    "        devices_logs = prep_logs.filter(prep_logs.event_name.isNotNull())\n",
    "        \n",
    "        # Агрегация таблицы    \n",
    "        devices_logs = aggregation_table(devices_logs)\n",
    "\n",
    "        devices_logs_null = prep_logs.filter(prep_logs.event_name.isNull())   \n",
    "        devices_logs_null = (devices_logs_null.join(yesterday_logs, \"external_did\")\n",
    "                                              .select(devices_logs_null.external_did, \n",
    "                                                      devices_logs_null.date, \n",
    "                                                      yesterday_logs.first_event_datetime,\n",
    "                                                      yesterday_logs.last_event_datetime,   \n",
    "                                                      yesterday_logs.push_token,\n",
    "                                                      yesterday_logs.ios_ifa,\n",
    "                                                      yesterday_logs.external_profile_id,\n",
    "                                                      yesterday_logs.external_app_id   \n",
    "                                                    )\n",
    "                                                )\n",
    "\n",
    "        devices_logs = devices_logs.union(devices_logs_null)\n",
    "        \n",
    "    # Сохранение таблицы\n",
    "    (\n",
    "    devices_logs.write\n",
    "                .mode(\"overwrite\")\n",
    "                .parquet(f\"{devices_path}/internal_app_id={internal_app_id}/{str(date)}\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица за предыдущий день не найдена\n",
      "-RECORD 0------------------------------------\n",
      " external_did         | 0378698611e8ab440... \n",
      " date                 | 2023-01-05           \n",
      " first_event_datetime | 2023-01-04 19:50:... \n",
      " last_event_datetime  | 2023-01-05 15:48:... \n",
      " push_token           | d-mIqF1ZlkB9io7uJ... \n",
      " ios_ifa              | null                 \n",
      " external_profile_id  | 3329a672-dc49-483... \n",
      " external_app_id      | application_1        \n",
      "-RECORD 1------------------------------------\n",
      " external_did         | 08a10c8277cbf2675... \n",
      " date                 | 2023-01-05           \n",
      " first_event_datetime | 2022-12-22 15:12:... \n",
      " last_event_datetime  | 2023-01-05 20:24:... \n",
      " push_token           | fp5UpE4pKkI9uiPJX... \n",
      " ios_ifa              | null                 \n",
      " external_profile_id  | 0ba29b76-a579-470... \n",
      " external_app_id      | application_1        \n",
      "-RECORD 2------------------------------------\n",
      " external_did         | 0c0ec224afb1502b2... \n",
      " date                 | 2023-01-05           \n",
      " first_event_datetime | 2022-11-06 21:12:... \n",
      " last_event_datetime  | 2023-01-05 13:49:... \n",
      " push_token           | eEuS0B350ETlgwX58... \n",
      " ios_ifa              | null                 \n",
      " external_profile_id  | 6d3ecf59-fd51-454... \n",
      " external_app_id      | application_1        \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "update_devices_table(\"1\", date(year=2023, month=1, day=5), \"Q:\\Python\\date\", \"Q:\\Python\\device\")\n",
    "\n",
    "df2 = spark.read.parquet(f\"Q:\\Python\\device\\internal_app_id=1\\{date(year=2023, month=1, day=5)}\")\n",
    "df2.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
